{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\" id=\"heading\">Sentiment Analysis of Reddit Data using Reddit API</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( ü§ó the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the session, you will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- know how to work with APIs\n",
    "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
    "- understand what a `pipeline` object is in HuggingFace\n",
    "- perform sentiment analysis using `pipeline`\n",
    "- run a python script in command line and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of each task, commit* the work into the repository you created before the assignment\n",
    "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your repository you created before the assignment\n",
    "- Submit the link to the notebook in Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task I: Instantiate a Reddit API Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please ensure you've ran all the cells in the `imports.ipynb`, located [here](https://github.com/FourthBrain/MLE-8/blob/main/assignments/week-3-analyze-sentiment-subreddit/imports.ipynb), to make sure you have all the required packages for today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2. Create a new app on Reddit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a Reddit account if you don't have one, log into your account.\n",
    "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
    "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect uri** ( The redirect URI is where the user is sent after they've granted OAuth access to your application (more info [here](https://github.com/reddit-archive/reddit/wiki/OAuth2)) For our purpose, you can enter some random url, e.g., www.google.com; as shown below.\n",
    "\n",
    "\n",
    "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
    "- Jot down `client_id` (left upper corner) and `client_secret` \n",
    "\n",
    "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
    "    \n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "- Create `secrets_reddit.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"client_id\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"secret_id\"\n",
    "    REDDIT_API_USER_AGENT = \"any string except bot; ex. My User Agent\"\n",
    "    ```\n",
    "- Add `secrets_reddit.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Instantiate a `Reddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import secrets_reddit\n",
    "\n",
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=secrets_reddit.REDDIT_API_CLIENT_ID,\n",
    "    client_secret=secrets_reddit.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent=secrets_reddit.REDDIT_API_USER_AGENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0x7f919b2307f0>\n"
     ]
    }
   ],
   "source": [
    "print(reddit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Instantiate a `subreddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected output you will see ar from `r/machinelearning` unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"machinelearning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the display name of the subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machinelearning'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    machinelearning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about its title, is it different from the display name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    Machine Learning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Print out the description of the subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
      "--------\n",
      "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
      "--------\n",
      "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
      "--------\n",
      "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
      "--------\n",
      "+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\n",
      "--------\n",
      "***[@slashML on Twitter](https://twitter.com/slashML)***\n",
      "--------\n",
      "***[Chat with us on Slack](https://join.slack.com/t/rml-talk/shared_invite/enQtNjkyMzI3NjA2NTY2LWY0ZmRjZjNhYjI5NzYwM2Y0YzZhZWNiODQ3ZGFjYmI2NTU3YjE1ZDU5MzM2ZTQ4ZGJmOTFmNWVkMzFiMzVhYjg)***\n",
      "--------\n",
      "**Beginners:**\n",
      "--------\n",
      "Please have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\n",
      "\n",
      "[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\n",
      "\n",
      "For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\n",
      "\n",
      "For career related questions, visit /r/cscareerquestions/\n",
      "\n",
      "--------\n",
      "\n",
      "[Advanced Courses (2016)](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\n",
      "\n",
      "[Advanced Courses (2020)](https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/)\n",
      "\n",
      "--------\n",
      "**AMAs:**\n",
      "\n",
      "[Pluribus Poker AI Team 7/19/2019](https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/)\n",
      "\n",
      "[DeepMind AlphaStar team (1/24//2019)](https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/)\n",
      "\n",
      "[Libratus Poker AI Team (12/18/2017)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/)\n",
      "\n",
      "[DeepMind AlphaGo Team (10/19/2017)](https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/)\n",
      "\n",
      "[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\n",
      "\n",
      "[Google Brain Team (8/11/2016)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\n",
      "\n",
      "[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\n",
      "\n",
      "[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\n",
      "\n",
      "[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\n",
      "\n",
      "[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\n",
      "\n",
      "[J√ºrgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\n",
      "\n",
      "[Geoffrey Hinton (11/10/2014)]\n",
      "(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\n",
      "\n",
      "[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\n",
      "\n",
      "[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\n",
      "\n",
      "[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\n",
      "\n",
      "--------\n",
      "Related Subreddit :\n",
      "\n",
      "* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\n",
      "\n",
      "* [Statistics](http://www.reddit.com/r/statistics)\n",
      "\n",
      "* [Computer Vision](http://www.reddit.com/r/computervision)\n",
      "\n",
      "* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\n",
      "\n",
      "* [NLP] (http://www.reddit.com/r/LanguageTechnology)\n",
      "\n",
      "* [ML Questions] (http://www.reddit.com/r/MLQuestions)\n",
      "\n",
      "* /r/MLjobs and /r/BigDataJobs\n",
      "\n",
      "* /r/datacleaning\n",
      "\n",
      "* /r/DataScience\n",
      "\n",
      "* /r/scientificresearch\n",
      "\n",
      "* /r/artificial\n"
     ]
    }
   ],
   "source": [
    "print(subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>\n",
    "\n",
    "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
    "    --------\n",
    "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
    "    --------\n",
    "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
    "    --------\n",
    "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
    "    --------\n",
    "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task II: Parse comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. Top Posts of All Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtime_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mgenerator_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a :class:`.ListingGenerator` for top items.\n",
      "\n",
      ":param time_filter: Can be one of: ``\"all\"``, ``\"day\"``, ``\"hour\"``,\n",
      "    ``\"month\"``, ``\"week\"``, or ``\"year\"`` (default: ``\"all\"``).\n",
      "\n",
      ":raises: :py:class:`ValueError` if ``time_filter`` is invalid.\n",
      "\n",
      "Additional keyword arguments are passed in the initialization of\n",
      ":class:`.ListingGenerator`.\n",
      "\n",
      "This method can be used like:\n",
      "\n",
      ".. code-block:: python\n",
      "\n",
      "    reddit.domain(\"imgur.com\").top(time_filter=\"week\")\n",
      "    reddit.multireddit(redditor=\"samuraisam\", name=\"programming\").top(time_filter=\"day\")\n",
      "    reddit.redditor(\"spez\").top(time_filter=\"month\")\n",
      "    reddit.redditor(\"spez\").comments.top(time_filter=\"year\")\n",
      "    reddit.redditor(\"spez\").submissions.top(time_filter=\"all\")\n",
      "    reddit.subreddit(\"all\").top(time_filter=\"hour\")\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/models/listing/mixins/base.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "# try run this line, what do you see? press q once you are done\n",
    "?subreddit.top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Project] From books to presentations in 10s with AR + ML\n",
      "[D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
      "[R] First Order Motion Model applied to animate paintings\n",
      "[N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
      "[D] This AI reveals how much time politicians stare at their phone at work\n",
      "[D] Types of Machine Learning Papers\n",
      "[D] The machine learning community has a toxicity problem\n",
      "I made a robot that punishes me if it detects that if I am procrastinating on my assignments [P]\n",
      "[Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
      "[P] Using oil portraits and First Order Model to bring the paintings back to life\n"
     ]
    }
   ],
   "source": [
    "for submission in subreddit.top(limit=10, time_filter=\"all\"):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details> <summary>Expected output:</summary>\n",
    "\n",
    "    [Project] From books to presentations in 10s with AR + ML\n",
    "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
    "    [R] First Order Motion Model applied to animate paintings\n",
    "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
    "    [D] This AI reveals how much time politicians stare at their phone at work\n",
    "    [D] Types of Machine Learning Papers\n",
    "    [D] The machine learning community has a toxicity problem\n",
    "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
    "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
    "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected output looks dated. The last couple of titles in the All time Top 10 list seem to have changed since this output was setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. Top 10 Posts of This Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P] Finetuned Diffusion: multiple fine-tuned Stable Diffusion models, trained on different styles\n",
      "[P] Transcribe any podcast episode in just 1 minute with optimized OpenAI/whisper\n",
      "[D] DALL¬∑E to be made available as API, OpenAI to give users full ownership rights to generated images\n",
      "[P] Made a text generation model to extend stable diffusion prompts with suitable style cues\n",
      "[R] APPLE research: GAUDI ‚Äî a neural architect for immersive 3D scene generation\n",
      "[P] Learn diffusion models with Hugging Face course üß®\n",
      "[R] Reincarnating Reinforcement Learning (NeurIPS 2022) - Google Brain\n",
      "[N] Adversarial Policies Beat Professional-Level Go AIs\n",
      "[P] Fine Tuning Stable Diffusion: Naruto Character Edition\n",
      "[N] Meta AI | Evolutionary-scale prediction of atomic level protein structure with a language model\n"
     ]
    }
   ],
   "source": [
    "for submission in subreddit.top(limit=10, time_filter=\"week\"):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details><summary>Expected output:</summary>\n",
    "\n",
    "    [N] Ian Goodfellow, Apple‚Äôs director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said ‚ÄúI believe strongly that more flexibility would have been the best policy for my team.‚Äù He was likely the company‚Äôs most cited ML expert.\n",
    "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
    "    [P] I‚Äôve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple‚Äôs version of MobileNet, and more directly on your phone's camera roll.\n",
    "    [R] Meta is releasing a 175B parameter language model\n",
    "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
    "    [P] T-SNE to view and order your Spotify tracks\n",
    "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
    "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
    "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
    "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 posts from last week is as expected very different from the expected output that was probably generated for the last cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "Check out what other attributes the `praw.models.Submission` class has in the [docs](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html). \n",
    "\n",
    "1. After having a chance to look through the docs, is there any other information that you might want to extract? How might this additional data help you?\n",
    "\n",
    "Write a sample piece of code below extracting three additional pieces of information from the submission below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each submission: extract Author, Score, No. of comments, and Submission date formatted\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "submission_dict = { \"title\":[], \"author\":[], \"score\":[], \"num_comments\":[], \"date\":[]}\n",
    "for submission in subreddit.top(limit=10, time_filter=\"week\"):\n",
    "        submission_dict[\"title\"].append(submission.title)\n",
    "        submission_dict[\"author\"].append(submission.author)\n",
    "        submission_dict[\"score\"].append(submission.score)\n",
    "        submission_dict[\"num_comments\"].append(submission.num_comments)\n",
    "        submission_dict[\"date\"].append(datetime.fromtimestamp(submission.created_utc))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[P] Finetuned Diffusion: multiple fine-tuned S...</td>\n",
       "      <td>Illustrious_Row_9971</td>\n",
       "      <td>1105</td>\n",
       "      <td>60</td>\n",
       "      <td>2022-11-05 03:17:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[P] Transcribe any podcast episode in just 1 m...</td>\n",
       "      <td>thundergolfer</td>\n",
       "      <td>430</td>\n",
       "      <td>23</td>\n",
       "      <td>2022-11-06 12:58:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[D] DALL¬∑E to be made available as API, OpenAI...</td>\n",
       "      <td>TiredOldCrow</td>\n",
       "      <td>411</td>\n",
       "      <td>59</td>\n",
       "      <td>2022-11-03 18:12:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[P] Made a text generation model to extend sta...</td>\n",
       "      <td>Neat-Delivery4741</td>\n",
       "      <td>395</td>\n",
       "      <td>55</td>\n",
       "      <td>2022-11-03 04:51:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[R] APPLE research: GAUDI ‚Äî a neural architect...</td>\n",
       "      <td>SpatialComputing</td>\n",
       "      <td>382</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-11-05 13:12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[P] Learn diffusion models with Hugging Face c...</td>\n",
       "      <td>lewtun</td>\n",
       "      <td>313</td>\n",
       "      <td>14</td>\n",
       "      <td>2022-11-04 08:28:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[R] Reincarnating Reinforcement Learning (Neur...</td>\n",
       "      <td>smallest_meta_review</td>\n",
       "      <td>247</td>\n",
       "      <td>31</td>\n",
       "      <td>2022-11-05 23:06:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[N] Adversarial Policies Beat Professional-Lev...</td>\n",
       "      <td>xutw21</td>\n",
       "      <td>169</td>\n",
       "      <td>49</td>\n",
       "      <td>2022-11-01 20:42:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[P] Fine Tuning Stable Diffusion: Naruto Chara...</td>\n",
       "      <td>mippie_moe</td>\n",
       "      <td>156</td>\n",
       "      <td>8</td>\n",
       "      <td>2022-11-03 10:52:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[N] Meta AI | Evolutionary-scale prediction of...</td>\n",
       "      <td>xutw21</td>\n",
       "      <td>110</td>\n",
       "      <td>20</td>\n",
       "      <td>2022-11-01 11:46:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                author  \\\n",
       "0  [P] Finetuned Diffusion: multiple fine-tuned S...  Illustrious_Row_9971   \n",
       "1  [P] Transcribe any podcast episode in just 1 m...         thundergolfer   \n",
       "2  [D] DALL¬∑E to be made available as API, OpenAI...          TiredOldCrow   \n",
       "3  [P] Made a text generation model to extend sta...     Neat-Delivery4741   \n",
       "4  [R] APPLE research: GAUDI ‚Äî a neural architect...      SpatialComputing   \n",
       "5  [P] Learn diffusion models with Hugging Face c...                lewtun   \n",
       "6  [R] Reincarnating Reinforcement Learning (Neur...  smallest_meta_review   \n",
       "7  [N] Adversarial Policies Beat Professional-Lev...                xutw21   \n",
       "8  [P] Fine Tuning Stable Diffusion: Naruto Chara...            mippie_moe   \n",
       "9  [N] Meta AI | Evolutionary-scale prediction of...                xutw21   \n",
       "\n",
       "   score  num_comments                date  \n",
       "0   1105            60 2022-11-05 03:17:11  \n",
       "1    430            23 2022-11-06 12:58:59  \n",
       "2    411            59 2022-11-03 18:12:45  \n",
       "3    395            55 2022-11-03 04:51:38  \n",
       "4    382             7 2022-11-05 13:12:14  \n",
       "5    313            14 2022-11-04 08:28:41  \n",
       "6    247            31 2022-11-05 23:06:06  \n",
       "7    169            49 2022-11-01 20:42:05  \n",
       "8    156             8 2022-11-03 10:52:09  \n",
       "9    110            20 2022-11-01 11:46:18  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_df = pd.DataFrame(submission_dict)\n",
    "submissions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "2. Is there any information available that might be a concern when it comes to Ethical Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P] DeepCreamPy - Decensoring Hentai with Deep Neural Networks\n"
     ]
    }
   ],
   "source": [
    "for submission in subreddit.top(time_filter=\"year\"):\n",
    "    if submission.over_18 == True:\n",
    "        print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears straightforward to get submissions that are over 18 with the API. A google safesearch would probably have not listed this title. I am not clear how the API ensures that such content is not presented by other means to under age users. I would be concerned about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. Comment Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "\n",
    "The purpose is \n",
    "1. to understand what the code is doing \n",
    "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later üòä) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 405 ms, sys: 238 ms, total: 644 ms\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Initialise an empty list for top comments \n",
    "top_comments = []\n",
    "\n",
    "# Iterate through the list of top 10 submissions\n",
    "for submission in subreddit.top(limit=10):\n",
    "    # Iterate through the comments for the current submission\n",
    "    for top_level_comment in submission.comments:\n",
    "        # Comments contain many \"Load More Comments...\" links, ignore them by skipping to end of loop\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # Add comments to list for top comments\n",
    "        top_comments.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the total number of comments for top 10 submissions. The num_comments includes the MoreComments. So count explicitly after removing MoreComments with replace_more(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check the total number of comments extracted with given code and with replace_more() matches: 746, 746, 2114\n"
     ]
    }
   ],
   "source": [
    "total_comments = 0\n",
    "total_comments_with_more = 0\n",
    "\n",
    "for submission in subreddit.top(limit=10):\n",
    "    total_comments_with_more += submission.num_comments\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for top_level_comment in submission.comments:\n",
    "        total_comments += 1\n",
    "    \n",
    "total_comments\n",
    "print(f\"Double check the total number of comments extracted with given code and with replace_more() matches: {len(top_comments)}, {total_comments}, {total_comments_with_more}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the num_comments is including the MoreComments in its count.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. Inspect Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many comments did you extract from the last step? Examine a few comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments)  # the answer may vary 693 for r/machinelearning - My answer is 746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Which type do you like the most?',\n",
       " 'Link?',\n",
       " 'well this is next gen future. i will take look on my grandpapa lol.',\n",
       " 'i hate the ones that begin with \"towards..\". why the tf would i want to read something that\\'s incomplete?',\n",
       " \"The first set of numbers was Yann LeCun's phone number at bell labs.\",\n",
       " \"And here I thought WinAmp visualizations really kicked the llama's ass... this explodes the llama into radioactive atoms\",\n",
       " \"> Thirdly, there is a worshiping problem.\\n\\ni agree about the godfathers portion.\\n\\nhowever the worship of publications from places like Google or DeepMind is unfortunately very well-founded.\\n\\nif you look at most university papers, they are training over 1/100th the amount of data industry papers use (for good reason).  as a practitioner it just isn't worth your time to look for other papers unless you're chasing the last few basis points.\",\n",
       " \"This is a good post and you're right, but there's one criticism I have:\\n\\n>**Sixthly**, moral and ethics are set *arbitrarily* ... At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care.\\n\\nThe way I see things, there is no such thing as evil knowledge. It's all just knowledge. The techniques, in a way, are there to be discovered whether we explore them or not.\\n\\nWhat's happening in China is horrifying and I'm sure a lot of us care. I just think you have to aim your ire in the right direction, though, which is at the people doing evil things with that knowledge, not the people uncovering the knowledge.\",\n",
       " 'Cant see his right hand',\n",
       " '[Posted on Reddit]']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "[random.choice(top_comments) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
    "\n",
    "    ['Awesome visualisation',\n",
    "    'Similar to a stack or connected neurons.',\n",
    "    'Will this Turing pass the Turing Test?']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "3. After having a chance to review a few samples of 5 comments from the subreddit, what can you say about the data? \n",
    "\n",
    "HINT: Think about the \"cleanliness\" of the data, the content of the data, think about what you're trying to do - how does this data line up with your goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len_comments = np.asarray([len(i.split()) for i in top_comments])\n",
    "print((len_comments > 500).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I totally agree with 99% of your stuff. All of them are great points.\n",
      "\n",
      "Although I will contest one of these points:\n",
      "\n",
      "> machine learning, and computer science in general, have a huge diversity problem\n",
      "\n",
      "I will say, in my experience, I did not find it to be particularly exclusionary.                      \n",
      "(I still agree on making the culture healthier and more welcoming for all people, but won't call it a huge diversity problem, that is any different from what plagues other fields)              \n",
      "I also think it has very little to do with those in CS or intentional rejection of minorities/women by CS as a field.\n",
      "\n",
      "Far fewer women and minorities enroll in  CS, so it is more of a highschool problem than anything. If anything, CS tries really really hard to hire and attract under represented groups into the fold. That it fails, does not necessarily mean it is exclusionary. Many other social factors tend to be at play behind cohort statistics. An ML person knows that better than anyone.               \n",
      "\n",
      "There is a huge push towards hiring black and latino people and women as well. Far more than any other STEM field. Anyone who has gone to GHC knows how much money is spent on trying to make CS look attractive to women. ( I support both initiatives, but I do think enough is being done) \n",
      "\n",
      "\n",
      "A few anecdotes from the hackernews thread the other day, as to greater social reasons for women not joining tech.\n",
      "\n",
      "Sample 1:\n",
      "\n",
      "> There's one other possible, additional reason.\n",
      "I recently asked a 17-year-old high school senior who is heading to college what she's planning to study, and she said it would be mathematics, biomedical engineering, or some other kind of engineering. She's self-motivated -- says she will be studying multi-variate calculus, PDEs, and abstract algebra on her own this summer. She maxed out her high school math curriculum, which included linear algebra as an elective.\n",
      "\n",
      ">Naturally, I asked her about computer science, and she said something like this (paraphrasing):\n",
      "\n",
      "> \"The kids who love computers at my high school seem to be able to spend their entire day focusing on a computer screen, even on weekends. I cannot do that. And those kids are mostly boys whose social behavior is a little bit on the spectrum.\"\n",
      "\n",
      "> While I don't fully agree with her perspective, it makes me wonder how many other talented people shun the field for similar reasons.\n",
      "\n",
      "Sample2: \n",
      "\n",
      "> My niece had almost the exact same opinion despite having multiple family members who didn't fit that description, including her mother! It wasn't until I introduced her to some of my younger female co-workers that she committed to being a CS major. She's now a third generation software engineer, which has to be fairly unique.\n",
      "\n",
      "> I've talked to her about it and she can't really articulate why. I'm closer to the nerd stereotype in that I'm on the computer a lot but her mother (my sister) definitely is not. I think it's mostly pop and teen culture still harboring the antisocial stigma. I'll have to talk to her some more.\n",
      "There is probably some connection with video games, in that boys overwhelmingly play games where girls do not. I don't think the games cause the disparity; whatever it is that draws boys to VGs is what draws them to CS as well\n",
      "\n",
      "You can't blame the field for being unable to fight off stigma imposed by 80-90s movies on an entire generations.\n",
      "\n",
      "For example, there is no dearth of Indian women in CS. (I think it is similar for Chinese people too). Both societies did not undergo the collective humiliation of nerds that the US went through, and CS is considered a respectable 'high status' field, where people of any personality type can gel in. Thus, women do not face the same kind of intimidation. This is a \"US high school and US culture\" problem. Not a CS problem. \n",
      "\n",
      "> Going on parental leave during a PhD or post-doc usually means the end of an academic career. \n",
      "\n",
      "To be fair, this is common to almost all academic fields. CS is no exception and I strongly support the having more accommodations for female employees in this regard. \n",
      "\n",
      "Honestly, look at almost all \"high stress, high workload\" jobs and men are over-represented in almost all areas. Additionally, they tend to be a very particular kind of obsessive \"work is life\" kind of men. While women are discouraged form having such an unhealthy social life, men are actively pushed in this direction by society. IMO, we should not be seeking equality by pushing women to abide by male stereotypes. Maybe, if CS became a little better for everyone, it would benefit all kinds of people who are seeking healthier lives, men and women alike. This actually flows quite well into your next point of \"cut-throat publish-or-perish mentality\".\n",
      " Quite right, for the most part.   \n",
      "\n",
      "\n",
      "1. There's no clear consensus for making papers publicly available while under submission. One one side, it means the research is not available while under review which kind of defeats the whole purpose of research (sharing it with everyone, and not sitting around 2-3 months). On the other hand, sharing it and making posts everywhere does compromise anonymity: even if the reviewers don't search explicitly for the paper, they 're highly likely to stumble upon it if their research lies in that area (arXiv update tweets, gs updates, RTs by people they follow, etc). I guess a straightforward solution would be to have a version of arXiv with higher anonymity, where author affiliation is revealed only after decisions (to the journal/conference to which that research is submitted) have been made. We need to think much more about this specific problem.   \n",
      "\n",
      "2. Reproducibility is indeed an issue. I honestly don't know why we're in 2020 and machine learning papers can still get away without providing code/trained models. Evaluating the trained model (which is, in the majority of ML related papers, the result) by the reviewers via an open-source system, perhaps like a test-bed specific for applications? For instance, evaluating the robustness of a model on Imagenet. This, of course, should happen along with making code both compulsory and running it as well. This may be a problem for RL related systems, but this doesn't mean we shouldn't even try doing this for any of the submissions.   \n",
      "\n",
      "3. Very true. For some part, it's the responsibility of organizers to not always run after the top 5-6 names, and include younger researchers to help audiences get familiar with a more diverse (and most times, interesting) set of research and ideas. For the other part, it is also up to the researchers to draw the line when they see themselves talking about the same slides at multiple venues over and over again.   \n",
      "\n",
      "4. This specific instance is somewhat debatable. Compared to the level of backlash and toxicity women and people of color receive online is not even close to what he did. Nonetheless, the discussion could be much cleaner.   \n",
      "\n",
      "5. I agree with the first half. I do see companies doing *something* about this, but surely not enough. Also, it's a bit sad/sketchy that most AI research labs do not openly release statistics about their gender/ethnicity distributions. \"People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem. \" There's a very clear difference between 'engage' and 'tone-police'. As long as you're doing the former, I don't see why you should be \"afraid\".  \n",
      " \n",
      "6. True (but isn't this a problem with nearly every field of science? Countless animals are mutilated and experimented upon in multiple ways for things as frivolous as hair gel) I guess, for instance, people working in NLP could be more careful (or rather, simply avoid) scraping Reddit to help stop the propagation of biases/hate, etc. Major face-recognition providing companies have taken steps to help curb the potential harms of AI, and there is surely scope for more.   \n",
      "\n",
      "7. \" Certain people submit 50+ papers per year to NeurIPS.\" I'd think most of such people would only be remotely associated with the actual work. Most students/researchers/advisors I know who work on a research project (either via actually leading it or a substantial amount of advising) have no more than 5-6 NeurIPS submissions a year? Nevertheless, universities should be a little relaxed about such 'count' based rules.   \n",
      "\n",
      "8. \"Everybody is under attack, but nothing is improved. \". It's not like Anandkumar woke up one fine day and said \"you know what? I hate LeCun\". Whatever the researchers in your examples have accused others of, it has been true for the most part. I don't see how calling out someone for sexist behavior by calling them 'sexist' is disrespectful if the person being accused quite visibly is. All of these instances may not directly be tied with research or our work, but it would be greatly ignorant to pretend that we all are just machines working on science, and have no social relations or interactions with anyone. The way you interact with people, the way they interact with you: everything matters. If someone gets called out for sexist behavior and we instantly run to defend such \"tags\" as \"disrespectful\", I don't see how we can solve the problem of representation bias in this community.  \n",
      "\n",
      "\n",
      "Also, kinda funny that a 'toxicity' related discussion is being started on Reddit. lol\n",
      "I would suggest the following to solve some of the issues.\n",
      "\n",
      "a)Community moderation on arxiv : We have upvotes, downvotes, comments, and ranking by hot, top, controversial on reddit and mods. This to a large extent enables reddit to be a place where you can voice your thoughts but someone can step in if a situation arises. I remember there was this huge backlash on a recent paper that talked about face detection to identify criminal behaviour. The authors were kind enough to retract their submission. Imagine if they had posted it on arxiv, was there anything anyone could have done about it? \n",
      "\n",
      "b)Set guidelines for arxiv: Imagine you get to review a paper but find out that it is from Geoff Hinton, or Yann LeCun.. would you be able to review it in an unbiased fashion? Maybe the authors could upload a blinded submission to arxiv and reveal the names once a) they decide to stop targetting a publication b) the draft gets accepted.\n",
      "\n",
      "c)Make Codes mandatory: The policy of code-release being optional was largely derived from the systems community where releasing the code meant revealing a lot of properiatary IPs (standard-cell libraries cost billions to model, RTL IP licenses were what earned companies money)..however even they have started gravitating towards open-source (if anyone is interested RISCV, tiny compiler by Austin Henley, JOS by MIT are great starting points) however, AI has started to go the other way, fortunately there are voices speaking out against it.\n",
      "\n",
      "d) Make ethics compulsory: There is this famous quote by Oppenheimer after they invented the Atom bomb: \"I am become death, the destroyer of worlds.\" AI researchers need to understand this quote applies a lot to them. \n",
      "The Atom Bomb killed around 126K people (lowest estimate) in a matter of minutes..Prior to that, if someone had to kill around 126K people, they would need an army that was at least twice that size and would need to fight for at least 20 months (US lost around 6,600 people a month during the war). \n",
      "Similarly, research that took around months/years can now be done in minutes/days. This is a tremendous amount of power and people who wield it can shape our future. It is thus important to focus on the \"ethics\" of AI rather than look at pure accuracy numbers.\n",
      "\n",
      "e) Better metrics: Increasingly there are models that are able to beat SOTA due to their sheer size. Take BERT for example, Do you think colleges in Africa, Asia would be able to afford the compute costs? How about we rank models based on cost (in terms of power consumed, in terms of money ) and not just based on accuracy?. \n",
      "\n",
      "f) While I might disagree with \"some\" of the language used by Gebru. She has a point. In an increasingly competitive world, if we choose not to stand up for those who do not have a voice, we are choosing to ignore their views and are complicit in silencing them. PhDs are toxic and cutthroat and AI research is even more so. My girlfriend was forced to walk out of a project for speaking out against harassment because the harasser was \"intelligent\". If people like Gebru are silenced, people like my girlfriend are the ones who will have to pay the price. I would highly recommend watching the documentary called \"disclosure\" on netflix to understand the consequences of ignoring someone's perspective. If Gebru hadn't spoken out against racism and the danger of facial recognition algorithms, we would still be having companies like clearview.ai mining our data for surveillance. \n",
      "\n",
      "g) Understand privilege: This is something ALL AI (and Security) researchers need to understand. If you are a researcher publishing one or two papers in AI (or Security), you have some degree of privilege. Think about what you need to know to be a decent AI researcher today: A fair deal of programming, linear algebra, probability, good vocabulary, free time to keep up with deluge of papers in your field, a good peer group to discuss and brainstorm ideas, and finally resources to conduct experiments. ALL of this is privilege. So when someone is trying to point out an issue, maybe we can listen.. and yes, sometimes the issue may not be presented correctly or the person might use language that we cannot stomach. But the question we must ask ourselves is \"What are we losing by just listening to the other person?\".\n",
      "I appreciate the directness of your points and I shall try (and, inevitably, fail) to emulate that in my response:\n",
      "\n",
      "> **First** of all, the peer-review process is *broken*.  \n",
      "\n",
      "It's the peer review system that is broken, it's just a disconnect between the traditional methods for publishing work and the way people actually share ideas and results. Traditional publishing is dead - it has been since the internet, and the final nail in the coffin was social media. Unfortunately, most of academic science has yet to move towards a good alternative. arXiv is one such glimmer on the horizon - instead of going through a slow month or year long process to share your ideas and findings - just self publish and truly allow your work to be judged by your peers (all of them). If it's true only a fourth of NeuIPS submissions are uploaded to arXiv then I am sad it's not far more. \n",
      "\n",
      "We need to integrate the countless new mediums of communication and visualization into how we share science. So far, it's mostly the large companies (OpenAI, Deepmind, etc) that present their work with blog posts including multimedia and even interactive visualizations. Instead of condensing everything down to an eight page static paper - we should be encouraging submissions of full multimedia websites - ideally built on a common framework to make powerful visualization tools available to everyone, automatically generate printable versions for the old timers, to enable reviewers to respond/discuss directly on the page (a la OpenReviews) and, if accepted, to base the acceptance on the submission hash to identify tampering and recognize later changes). I am not advocating eliminating the peer-review system, I want make the whole process as transparent and dynamic as possible, and integrate the newest tools and media available.\n",
      "\n",
      "> **Secondly,** there is a *reproducibility crisis* \n",
      "\n",
      "I agree, that's why we need a fundamentally new publishing platform where we can integrate/share code, data, and models directly (rather than occasionally linking to a disparate github repo). Ideally, the framework would have some compute behind it (maybe like Google Colab) so that all the models and code submitted can be run directly in the conference/journal submission page - imagine that: reviewers being able to interactively test people's models rather than going off of nothing but cherry-picked samples.\n",
      "\n",
      "> **Thirdly,** there is a *worshiping* problem. \n",
      "\n",
      "That's probably true, although I can't speak too much about it, as I'm not very involved in the politics of academic research. That being said, all enterprises will inevitably involve some icky politics and favoritism. The best we can do to combat that is make things as transparent as possible.\n",
      "\n",
      "> **Fourthly**, ... *toxicity*\n",
      "\n",
      " This problem goes far being ML/AI research - it has pretty much pervaded throughout all of public discourse at this point. We can discuss the problem of \"toxicity\" in general, which I would chalk up to our culture having not quite come to terms with the fundamentally different way we have to process information in the information age. However, overall I think AI research (and science in general) does a better job than most areas on that front.\n",
      "\n",
      "> **Fifthly**, ... a huge *diversity problem* \n",
      "\n",
      "I completely agree, and there are plenty of arguments for why we and all of academia has a diversity problem. You are probably familiar with most, and we don't have to get into them, but suffice to say, once again, our cultural and traditional biases and institutions conflict with a more contemporary mentality. What do we do about it? Outreach and transparency - they are slow but they work.\n",
      "\n",
      "> **Sixthly**, moral and ethics are set *arbitrarily* \n",
      "\n",
      "The problem here is a little unclear to me? Is it that people use technology in ways other people don't like? That seems inevitable. Is it that Western culture undervalues the rest of the world? What else is new? Don't get dragged down with the American Exceptionalists in denial as the US heads for economic and social stagnation and decline.\n",
      "\n",
      "> **Seventhly**, there is a cut-throat publish-or-perish *mentality*.  \n",
      "\n",
      "Coming from physics research, I agree that the AI field has a dangerously strong publish-or-perish mentality. However, that also means the field is highly dynamic and garners lots of interest/funding. I'm not convinced that a field as closely intertwined with engineering and the private sector does not actually benefit from a shorter project cycle. Additionally, the barrier to entry is virtually non-existent (unlike most other sciences where researchers won't give you the time of day if you don't already have a PhD, and the equipment/expertise necessary for making progress precludes anyone outside of 2-3 groups on Earth from publishing on your topic).\n",
      "\n",
      "> **Finally**, discussions have become *disrespectful*. \n",
      "\n",
      "Again, that's really just a misunderstanding for the way information works in the information age. Attention is a commodity and insulting people still has a high rate of return. This will change for the better as we get a handle on how to process information in this brave new world (especially in informal settings like social media).\n",
      "\n",
      "Thanks for the points though - it does us well to think critically about not just the \"what\" in research but also the \"how\" and \"why\".\n",
      "This post is a grab-bag of unrelated, tired (even if valid) complaints about the field.\n",
      "\n",
      ">First of all, the peer-review process is broken.\n",
      "\n",
      "First of all, what does this have to do with toxicity? \n",
      "\n",
      ">Every fourth NeurIPS submission is put on arXiv. \n",
      "\n",
      "The fact that papers are going up on arXiv is a good thing. The fact that peer-review suffers as a result is bad, and it has been raised and discussed many times but no one yet has a solution. The fact is that we don't currently have a system that both allows for fast dissemination of research and a blind review process. That it has not been fixed is not for the lack of attention or trying.\n",
      "\n",
      ">Secondly, there is a reproducibility crisis.\n",
      "\n",
      "Secondly, what does this have to do with toxicity? \n",
      "\n",
      ">Papers that do not beat the current state-of-the-art method have a zero chance of getting accepted at a good conference.\n",
      "\n",
      "This is patently false, and one of many instances of hyperbole in this post.\n",
      "\n",
      "People have been discussing a \"reproducability crisis\" in ML, but... where is it? The BERT-class models in ML have been consistently reproduced. To my knowledge the best vision models have similarly had their results reproduced too. Where there's an unreproducable result, it's either been called out and the author responds, or without a response it's taken as an unreproducable result that's ignored. The biggest reproducability problem has to do with access to data and computational resources, but that's by no means the same \"reproducability crisis\" in other fields.\n",
      "\n",
      ">Thirdly, there is a worshiping problem. Every paper with a Stanford or DeepMind affiliation gets praised like a breakthrough.\n",
      "\n",
      "More hyperbole.\n",
      "\n",
      ">BERT has seven times more citations than ULMfit\n",
      "\n",
      "It also performs a lot better than ULMFiT. I say this as someone who thinks ULMFiT doesn't get enough spotlight in the LM->encoder sphere. ELMo also basically disappeared overnight because of BERT.\n",
      "\n",
      ">Next, Bengio, Hinton, and LeCun are truly deep learning pioneers but calling them the \"godfathers\" of AI is insane. It has reached the level of a cult.\n",
      "\n",
      "Can you explain, in concrete terms, how using an analogy of \"godfather\" (which I take in the meaning of a founding leading, rather than from the mafia) is \"insane\" and \"has reached the level of a cult\"? Or is that just hyperbole?\n",
      "\n",
      ">Fourthly, the way Yann LeCun talked about biases and fairness topics was insensitive. However, the toxicity and backlash that he received are beyond any reasonable quantity.\n",
      "\n",
      "This is the first actual mention of toxicity, and very obviously the trigger for you to rant about the field.\n",
      "\n",
      ">Fifthly, machine learning, and computer science in general, have a huge diversity problem. \n",
      "\n",
      "Let may state this first, and upfront, that while this problem is not unique to ML and CS, it is still an important problem that needs to be addressed. That said, it has nothing to do with toxicity (or specifically, not in the way you're talking about. You're not, for example, talking about how toxicity makes ML less diverse, you're in fact arguing the opposite) and it sounds like just another point to pad out your list of complaints, until:\n",
      "\n",
      ">this lack of diversity is often abused as an excuse to shield certain people from any form of criticism. Reducing every negative comment in a scientific discussion to race and gender creates a toxic environment. People are becoming afraid to engage in fear of being called a racist or sexist, which in turn reinforces the diversity problem.\n",
      "\n",
      "I have no idea what on earth you are talking about, or where you are getting into these sort of discussions.\n",
      "\n",
      ">Sixthly, moral and ethics are set arbitrarily. The U.S. domestic politics dominate every discussion. At this very moment, thousands of Uyghurs are put into concentration camps based on computer vision algorithms invented by this community, and nobody seems even remotely to care. \n",
      "\n",
      "You've just described... the Internet in general. Look at the front page of Reddit: it is just as dominated by US politics. Same for Twitter trending. \n",
      "\n",
      ">Seventhly, there is a cut-throat publish-or-perish mentality. If you don't publish 5+ NeurIPS/ICML papers per year, you are a looser.\n",
      "\n",
      "I have never seen someone publicly called a loser for not publishing sufficiently. I'm sure it's happened in specific groups, but it is not generally considered acceptable by the community. That's even putting aside the hyperbole of \"publish 5+ NeurIPS/ICML papers per year\".\n",
      "\n",
      "Also, what does this have to do with toxicity?\n",
      "\n",
      ">Finally, discussions have become disrespectful. ... Gebru calls LeCun a white supremacist\n",
      "\n",
      "Gebru never called LeCun a white supremacist. Did you distort what she said for the purpose of fanning flames of an argument? Is that the not clearest possible example of \"toxicity\" you are arguing against?\n",
      "\n",
      "----\n",
      "\n",
      "The field is not without its problems, for sure. There are many issues of accessibility, diversity, and dissemination of information that need to be addressed. Most of it has to do with how quickly the field has grown, and the institutions and even social conventions that have not yet adjusted to accommodate its new size and prominence (too many qualified students, too many papers, too many new results). A related part of it is the potential misuse of the technology that we're building and researching. And for all the negativity and online arguments that have gotten of hand, one of the best parts of the field is that a lot of this is done in public, with free communication, and a lot of genuine self-criticism. Ask almost anyone in the field and they would agree that we are not doing enough to address all of these problems, even if we don't yet agree on how we can do better.\n",
      "\n",
      "Posting a big list of unrelated, hyperbolic complaints stemming from cherry-picked examples (How many labs have PIs who don't know all their PhD students? How often do researchers publicly go after their reviewers?) for the purpose of stirring up a big flamey debate, does nothing to help. You're picking out the worst possible examples to [mischaracterize the field and the community](https://www.reddit.com/r/MachineLearning/comments/hiv3vf/d_the_machine_learning_community_has_a_toxicity/fwiikfx/). If you wanted to have an actual discussion on toxicity, you would have focused on that rather than include a load of unrelated points to make your big rant.\n",
      "\n",
      "Signing off your message with \"Best intentions\" does not excuse the rest of your post. Based on your post history I think you do have good intentions but this post is absolutely not productive.\n",
      "There  is a huge toxicity problem in the field indeed -- credit assignment is  at the top of the list (and I am not talking about the credit assignment  problem in artificial neural networks...).\n",
      "\n",
      "As  a scientist/researcher/professor in the field myself, I have witnessed a wide  variety of issues ranging from the unethical rejection of  papers from  conferences such as NIPS/ICML and the very broken ICLR to the  exploitation of \"noise in the review system\", where researchers just  keep submitting the same paper across conferences until they sample the  right set of reviewers who will accept their paper, to plagiarism (and  more commonly, \"idea plagiarism\").(With respect to ICLR, the  concepts behind OpenReview are good in theory, and I understand its  ideals, but it is implemented poorly in practice, in my opinion.  Ultimately, this creates what I call the \"wall of shame\" for papers that  have been rejected, making it difficult for graduate students and  researchers to overcome the bad reviews received -- and this is made  worse b/c the reviewers are kept anonymous and thus not held accountable  to their poor reviews).\n",
      "\n",
      "Ultimately,  what has been created in the field in many ways is what I have called  for many years the \"deep learning rat race\", where accomplishments are  often just outperforming a benchmark by a percentage point or two.   Furthermore, the review process is not being held to higher standards  (often attributed to the increasing deluge of submissions that place a  tremendous burden on reviewers and conference staff), leading to  situations where some actually reject a paper and then \"copy\" the idea  for themselves (with no citation at minimum -- again, the \"credit  assignment\" problem as noted above) in their own work (and if the copier  comes from a prestigious lab, the original source/proposer gets  overshadowed since they do not have the prestige of name that comes with  Stanford or Mila, for example).\n",
      "\n",
      "I  could go on further and add plenty of details and \"war stories\" to  accompany some of the issues I have raised above (and this does not even  address the many other problems pointed out in the OP's post). But, in  essence, I think that what the machine learning community, at large, really needs is a drastic \"culture change\" across all levels (ranging from the  newcomers to the famous/established) addressing problems that plague the  field such as \"publish or perish\" and \"idea plagiarism\" (prominent in  the famous/big labs especially) as well as reviewing quality in  conferences.I often find much better reviewing (in general, there  are exceptions) in journals as opposed to conferences, where at least  the researcher is given reasonable and useful constructive feedback that  can be used to improve the paper and address issues in the work (if  they are addressable). Conferences have, especially recently, become a  disappointment for me, more than usual, given that the reviewers will  not even read the rebuttals me and my students carefully craft to abide  by the very strong constraints on word/character limits while still  addressing issues from reviewers that are actually address clearly in  the very text of the paper (of course, this assumes reviewers read the  whole paper -- which is unlikely, given that so their plate is quite  full with many, many reviews overall). Until we induce a deep cultural  shift in the field of machine learning and truly address its \"old boys'  club\" like scheme (where only those coming from the prestige get their  work recognized), the field will only progress more slowly.\n",
      "\n",
      "I  will mention though (for fellow professors that share my silent agony),  that part of this change comes from within our own labs. While it is slow  and more challenging to change our institutions, instilling a strong and  healthy culture and set of practices in one's own lab is key to  inducing the cultural shift I wish would happen across the field  globally. If you hold your students to rigor and credit assignment,  build lab comradery (starting by knowing the names of your students, at  the very minimum) and supporting your students whenever you face often  cruel and unethical rejections, and never let your own work slip due to  the many frustrations and issues from the field, I believe your lab can  contribute to a brighter future.\n"
     ]
    }
   ],
   "source": [
    "for c in top_comments: \n",
    "    if(len(c.split()) > 500): \n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume our goal is to analyse the sentiment of responses to a particular submission on Reddit. Here are some observations about the comments: \n",
    "* There are conversational abbreviations - we need to ensure the dataset our model is trained on include such abbreviations like pls, sry, imho etc. Sometimes people abuse when they are excited! E.g. 'Wtffff. Well that was incredible.' And this: 'gonna take shrooms then revisit this.' Wonder if the sentiment analyser will capture the sentiment here! \n",
    "* Some posts are very long (as shown above) and contain both criticism as well as , and it is difficult to classify them into positive or negative. I suspect they will go into neutral category, but we would have lost out on all the different sentiments in that comment. There are 6 comments that are more than 500 words! \n",
    "* There is a lot of markdown formatting in the text with \\*\\ \\\\n etc. Some preprocessing will be needed to remove these from the text. E.g. 'Make the robot \\*later\\*.','\\\\>Schmidhuber calls Hinton a thief,\\n\\nNo doubt Hinton is a thief, the whole Toronto communities are thieves and gangsta.Hinton community cross site every stupid articles they write.',  '[deleted]' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "subreddit_tsla = reddit.subreddit(\"TSLA\")\n",
    "top_comments_tsla = []\n",
    "for submission in subreddit_tsla.top(limit=10, time_filter=\"year\"):\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for top_level_comment in submission.comments:\n",
    "        top_comments_tsla.append(top_level_comment.body)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments_tsla) # Expected: 174 for r/machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I set my buy target‚Ä¶. At $420 per share.',\n",
       " 'Nice I‚Äôm hoping to jump in after the split. Is there any recommendations how long after the split to jump in',\n",
       " 'Since fractional shares can be bought, the price of of a stock is irrelevant. It will still earn or lose the same. It is purely psychological.',\n",
       " 'When‚Äôs this stock split happening ?',\n",
       " 'Nope I wussed out with the economy and Russia. And to top it off I bought a lot near the peak- $1052 and sold it off well under that. \\n\\nFortunately I lost $50 only. I bought/sold more as it made its descent into hell and back. \\n\\nUnfortunately I should have held when it was at below $800. \\n\\nI‚Äôll buy it. But not at $1000.  I simply cannot imagine this going to $2000 with all that‚Äôs going outside of Tesla‚Äôs control. \\n\\nI‚Äôll look at $900, maybe dip in at $800 and love it at $700.',\n",
       " 'I gave up on Elon shit... but wish you luck',\n",
       " 'TSLA to $3000',\n",
       " \"Second time in as many weeks this moron has tweeted something to fuck the stock price. Im getting tired of this shit.\\n\\nI've been long since 2015 and it's probably time I just close out my position.\",\n",
       " \"Second time in as many weeks this moron has tweeted something to fuck the stock price. Im getting tired of this shit.\\n\\nI've been long since 2015 and it's probably time I just close out my position.\",\n",
       " 'Price predication for next year after split?']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(top_comments_tsla) for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
    "\n",
    "    ['I bought puts',\n",
    "    '100%',\n",
    "    'Yes. And I‚Äôm bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "4. Now that you've had a chance to review another subreddits comments, do you see any differences in the kinds of comments either subreddit has - and how might this relate to bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments in r/TSLA are all related to the stock prices of Tesla shares. So the language is very specific to stock trading. If the dataset on which the sentiment analyser is trained is representative of this type of language usage, then the pre-trained model will perform reasonably well. But this is very specific/niche usage. So I suspect that the pre-trained model will need to be trained on online discussions around stock trading to better classify sentiments in this space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task III: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Import `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\") # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "comment = random.choice(top_comments_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sold my last position at $1k. Have been waiting to jump back in finally did it 8 shares at $640'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Make Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.989326000213623}]\n"
     ]
    }
   ],
   "source": [
    "sentiment = sentiment_model([\"Bury Burry!!!!!\"])# YOUR CODE HERE\n",
    "print(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the type of the output `sentiment`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "[{'label': 'NEGATIVE', 'score': 0.989326000213623}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment: Sold my last position at $1k. Have been waiting to jump back in finally did it 8 shares at $640\n",
      "Predicted Label is NEGATIVE and the score is 0.989\n"
     ]
    }
   ],
   "source": [
    "print(f'The comment: {comment}')\n",
    "print(f'Predicted Label is {sentiment[0][\"label\"]} and the score is {sentiment[0][\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example comment, the output is:\n",
    "\n",
    "    The comment: Bury Burry!!!!!\n",
    "    Predicted Label is NEGATIVE and the score is 0.989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üñ•Ô∏è‚ùì Model Question:\n",
    "\n",
    "1. What does the score represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score represents the confidence of the prediction as POSITIVE or NEGATIVE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Put All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the piece together, create a simple script that does \n",
    "\n",
    "- get the subreddit\n",
    "- get comments from the top posts for given subreddit\n",
    "- run sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top_tlsa_comment_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_tlsa_comment_sentiment.py\n",
    "\n",
    "import secrets_reddit as secrets\n",
    "import random\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.subreddit import Subreddit\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_subreddit(display_name:str) -> Subreddit:\n",
    "    \"\"\"Get subreddit object from display name\n",
    "\n",
    "    Args:\n",
    "        display_name (str): [description]\n",
    "\n",
    "    Returns:\n",
    "        Subreddit: [description]\n",
    "    \"\"\"\n",
    "    reddit = Reddit(\n",
    "        client_id=secrets.REDDIT_API_CLIENT_ID,        \n",
    "        client_secret=secrets.REDDIT_API_CLIENT_SECRET,\n",
    "        user_agent=secrets.REDDIT_API_USER_AGENT\n",
    "        )\n",
    "    \n",
    "    subreddit = reddit.subreddit(display_name) # YOUR CODE HERE\n",
    "    return subreddit\n",
    "\n",
    "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
    "    \"\"\" Get comments from subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit (Subreddit): [description]\n",
    "        limit (int, optional): [description]. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of comments\n",
    "    \"\"\"\n",
    "    top_comments = []\n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            top_comments.append(top_level_comment.body)\n",
    "    return top_comments\n",
    "\n",
    "def run_sentiment_analysis(comment:str) -> Dict:\n",
    "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
    "    \n",
    "    Args:\n",
    "        comment (str): [description]\n",
    "        \n",
    "    Returns:\n",
    "        str: Sentiment analysis result\n",
    "    \"\"\"\n",
    "    sentiment_model = pipeline(\"sentiment-analysis\") # YOUR CODE HERE\n",
    "    sentiment = sentiment_model(comment)\n",
    "    return sentiment[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    subreddit = get_subreddit(\"TSLA\")\n",
    "    comments = get_comments(subreddit)\n",
    "    comment = random.choice(comments)# YOUR CODE HERE\n",
    "    sentiment = run_sentiment_analysis(comment)\n",
    "    \n",
    "    print(f'The comment: {comment}')\n",
    "    print(f'Predicted Label is {sentiment[\"label\"]} and the score is {sentiment[\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "The comment: Have you ever talked with Elon? Could say few words about him? Thanks\n",
      "Predicted Label is POSITIVE and the score is 0.999\n"
     ]
    }
   ],
   "source": [
    "!python top_tlsa_comment_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Expected output:</summary>\n",
    "\n",
    "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "    The comment: When is DOGE flying\n",
    "    Predicted Label is POSITIVE and the score is 0.689\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "5. Is the subreddit active? About how many posts or threads per day? How could you find this information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts: Machine Learning 89 vs TSLA 7 last week\n",
      "Comments: Machine Learning 908 vs TSLA 48 last week\n"
     ]
    }
   ],
   "source": [
    "num_posts_ml = 0 \n",
    "num_comments_ml = 0\n",
    "for submission in subreddit.top(limit=100, time_filter=\"week\"):\n",
    "    #print(f\"{submission.title}, {submission.author}, {submission.num_comments}\")\n",
    "    num_comments_ml += submission.num_comments\n",
    "    num_posts_ml += 1\n",
    "\n",
    "\n",
    "num_posts_tsla = 0 \n",
    "num_comments_tsla = 0\n",
    "for submission in subreddit_tsla.top(limit=100, time_filter=\"week\"):\n",
    "    #print(f\"{submission.title}, {submission.author}, {submission.num_comments}\")\n",
    "    num_comments_tsla += submission.num_comments\n",
    "    num_posts_tsla += 1\n",
    "\n",
    "print(f\"Posts: Machine Learning {num_posts_ml} vs TSLA {num_posts_tsla} last week\")\n",
    "print(f\"Comments: Machine Learning {num_comments_ml} vs TSLA {num_comments_tsla} last week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "Redirect",
     "evalue": "Redirect to /r/MachineLearning/login/ (You may be trying to perform a non-read-only action via a read-only instance.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRedirect\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/rnirms/Documents/Work/mle-course/code/MLE-10/assignments/week-03-reddit-sentiment-analysis/nb/analyze-sentiment-subreddit.ipynb Cell 104\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rnirms/Documents/Work/mle-course/code/MLE-10/assignments/week-03-reddit-sentiment-analysis/nb/analyze-sentiment-subreddit.ipynb#Y220sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stats \u001b[39m=\u001b[39m subreddit\u001b[39m.\u001b[39;49mtraffic()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rnirms/Documents/Work/mle-course/code/MLE-10/assignments/week-03-reddit-sentiment-analysis/nb/analyze-sentiment-subreddit.ipynb#Y220sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stats\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/models/reddit/subreddit.py:1548\u001b[0m, in \u001b[0;36mSubreddit.traffic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraffic\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, List[List[\u001b[39mint\u001b[39m]]]:\n\u001b[1;32m   1523\u001b[0m     \u001b[39m\"\"\"Return a dictionary of the :class:`.Subreddit`'s traffic statistics.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \n\u001b[1;32m   1525\u001b[0m \u001b[39m    :raises: ``prawcore.NotFound`` when the traffic stats aren't available to the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1546\u001b[0m \n\u001b[1;32m   1547\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1548\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reddit\u001b[39m.\u001b[39;49mget(API_PATH[\u001b[39m\"\u001b[39;49m\u001b[39mabout_traffic\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mformat(subreddit\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/reddit.py:634\u001b[0m, in \u001b[0;36mReddit.get\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[39m@_deprecate_args\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m     params: Optional[Union[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m ):\n\u001b[1;32m    628\u001b[0m     \u001b[39m\"\"\"Return parsed objects returned from a GET request to ``path``.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[39m    :param path: The path to fetch.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[39m    :param params: The query parameters to add to the request (default: ``None``).\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_objectify_request(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, path\u001b[39m=\u001b[39;49mpath)\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/reddit.py:739\u001b[0m, in \u001b[0;36mReddit._objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_objectify_request\u001b[39m(\n\u001b[1;32m    714\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    715\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m     path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    722\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    723\u001b[0m     \u001b[39m\"\"\"Run a request through the ``Objector``.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \n\u001b[1;32m    725\u001b[0m \u001b[39m    :param data: Dictionary, bytes, or file-like object to send in the body of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m \n\u001b[1;32m    737\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objector\u001b[39m.\u001b[39mobjectify(\n\u001b[0;32m--> 739\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    740\u001b[0m             data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    741\u001b[0m             files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    742\u001b[0m             json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    743\u001b[0m             method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    744\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    745\u001b[0m             path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[39m=\u001b[39m _generate_arg_string(_old_args[: \u001b[39mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPositional arguments for \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m will no longer be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m supported in PRAW 8.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCall this function with \u001b[39m\u001b[39m{\u001b[39;00marg_string\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(_old_args, args)), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/praw/reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[39mraise\u001b[39;00m ClientException(\u001b[39m\"\u001b[39m\u001b[39mAt most one of `data` or `json` is supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    940\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_core\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    942\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    943\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    944\u001b[0m         json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    945\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    946\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    947\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m \u001b[39mexcept\u001b[39;00m BadRequest \u001b[39mas\u001b[39;00m exception:\n\u001b[1;32m    950\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/prawcore/sessions.py:330\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m url \u001b[39m=\u001b[39m urljoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requestor\u001b[39m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 330\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_with_retries(\n\u001b[1;32m    331\u001b[0m     data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    332\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    333\u001b[0m     json\u001b[39m=\u001b[39;49mjson,\n\u001b[1;32m    334\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    335\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    336\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    337\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    338\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sa/lib/python3.8/site-packages/prawcore/sessions.py:266\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_retry(\n\u001b[1;32m    254\u001b[0m         data,\n\u001b[1;32m    255\u001b[0m         files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m         url,\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    265\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[39m.\u001b[39mstatus_code](response)\n\u001b[1;32m    267\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m codes[\u001b[39m\"\u001b[39m\u001b[39mno_content\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mRedirect\u001b[0m: Redirect to /r/MachineLearning/login/ (You may be trying to perform a non-read-only action via a read-only instance.)"
     ]
    }
   ],
   "source": [
    "stats = subreddit.traffic()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = subreddit.public_traffic\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea on how active the subreddit is by looking at the no. of posts and comments in the last week or last day. From a quick count, it appears that the Machine Learning subreddit is more active than TSLA, with more posts and comments. \n",
    "\n",
    "Posts: Machine Learning 89 vs TSLA 7 last week\n",
    "\n",
    "Comments: Machine Learning 908 vs TSLA 48 last week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "6. Does there seem to be a large distribution of posters or a smaller concentration of posters who are very active? What kind of impact might this have on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Illustrious_Row_9971': 7, 'Singularian2501': 6, 'xutw21': 3, 'SpatialComputing': 3, 'MysteryInc152': 2, 'hardmaru': 2, 'Effective_Tax_2096': 2, 'mippie_moe': 2, 'cloneofsimo': 2, 'lexfridman': 1, 'Mogady': 1, 'TiredOldCrow': 1, 'jsonathan': 1, 'Neat-Delivery4741': 1, 'highergraphic': 1, 'pommedeterresautee': 1, 'lewtun': 1, 'MLC_Money': 1, 'jkterry1': 1, 'dilmerv': 1, 'smallest_meta_review': 1, 'obsoletelearner': 1, 'Greedy_Childhood8732': 1, 'Lajamerr_Mittesdine': 1, 'That_Violinist_18': 1, '0xWTC': 1, 'thundergolfer': 1, 'ggerganov': 1, 'aviisu': 1, 'likeamanyfacedgod': 1, 'shahaff32': 1, 'st8ic': 1, 'SleekEagle': 1, '0x00groot': 1, 'fromnighttilldawn': 1, 'YaYaLeB': 1, 'CodingButStillAlive': 1, 'DisWastingMyTime': 1, 'MohamedRashad': 1, 'TallTahawus': 1, 'laprika0': 1, 'fedetask': 1, 'killver': 1, 'ChrisRackauckas': 1, 'Technical-Vast1314': 1, 'Batuhan_Y': 1, 'lifesthateasy': 1, 'vajraadhvan': 1, 'Wiskkey': 1, 'tuned-mec-is': 1, 'ZeronixSama': 1, 'aozorahime': 1, 'phraisely': 1, 'No_Captain_856': 1, 'CauseRevolutionary59': 1, 'big_dog_2k': 1, 'Visual-Arm-7375': 1, 'hapliniste': 1, 'Signal-Mixture-4046': 1, 'dasayan05': 1, 'FerretDude': 1, 'Ok-Alps-7918': 1, 'zuccoff': 1, 'DragonLord9': 1, 'anqmj': 1, 'zergling103': 1, 'windoze': 1, 'xl0': 1, 'gdemarcq': 1, 'adasken': 1, 'lennart-reiher-ika': 1, 'pranftw': 1, 'Disastrous_Expert_22': 1, 'jd_3d': 1, 'tobyoup': 1, '4bedoe': 1, 'eparlan': 1, 'nullspace1729': 1, 'External_Oven_6379': 1, 'Living_Impression_37': 1}\n",
      "No. of posters in last month: 80\n",
      "No. of posts in last month: 100\n",
      "Mean no. of posts per author: 1.25\n",
      "Median no. of posts per author: 1.0\n"
     ]
    }
   ],
   "source": [
    "posters = {}\n",
    "\n",
    "for submission in subreddit.top(limit=100, time_filter=\"month\"):\n",
    "    author_name = submission.author.name\n",
    "    if author_name in posters:\n",
    "        num_posts = posters.get(author_name)\n",
    "        num_posts += 1\n",
    "    else:\n",
    "        num_posts = 1\n",
    "\n",
    "    posters.update({author_name: num_posts})\n",
    "\n",
    "sorted_posters_by_posts = dict(sorted(posters.items(), key=lambda x:x[1], reverse=True))\n",
    "print(sorted_posters_by_posts)\n",
    "print(f\"No. of posters in last month: {len(sorted_posters_by_posts)}\")\n",
    "print(f\"No. of posts in last month: {np.sum(list(sorted_posters_by_posts.values()))}\")\n",
    "print(f\"Mean no. of posts per author: {np.mean(list(sorted_posters_by_posts.values()))}\")\n",
    "print(f\"Median no. of posts per author: {np.median(list(sorted_posters_by_posts.values()))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wewewawa': 19, 'revanold': 8, 'droneauto': 2, 'TonyLiberty': 2, 'PrimaryMysterious': 1, 'LoganLee43': 1, 'ecoshares': 1, 'Local-Rip9621': 1, 'MoneyAdx': 1, 'Plus_Seesaw2023': 1, 'PolarBearPolo': 1, 'trowawayfarawaytoday': 1, 'Super_Stickman13': 1, 'EqualFlower': 1, 'JamesJimmyJim': 1, 'trillamanillla': 1, 'SouthSink1232': 1, 'Relative-Addendum534': 1, 'mbj7000': 1}\n",
      "No. of posters in last month: 19\n",
      "No. of posts in last month: 46\n",
      "Mean no. of posts per author: 2.4210526315789473\n",
      "Median no. of posts per author: 1.0\n"
     ]
    }
   ],
   "source": [
    "posters = {}\n",
    "\n",
    "for submission in subreddit_tsla.top(limit=100, time_filter=\"month\"):\n",
    "    author_name = submission.author.name\n",
    "    if author_name in posters:\n",
    "        num_posts = posters.get(author_name)\n",
    "        num_posts += 1\n",
    "    else:\n",
    "        num_posts = 1\n",
    "\n",
    "    posters.update({author_name: num_posts})\n",
    "\n",
    "sorted_posters_by_posts = dict(sorted(posters.items(), key=lambda x:x[1], reverse=True))\n",
    "print(sorted_posters_by_posts)\n",
    "print(f\"No. of posters in last month: {len(sorted_posters_by_posts)}\")\n",
    "print(f\"No. of posts in last month: {np.sum(list(sorted_posters_by_posts.values()))}\")\n",
    "print(f\"Mean no. of posts per author: {np.mean(list(sorted_posters_by_posts.values()))}\")\n",
    "print(f\"Median no. of posts per author: {np.median(list(sorted_posters_by_posts.values()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above results, the MachineLearning subreddit has a better distribution of posters - with mean no. of posts at 1.25 that is close to the median of 1 post per poster. The TSLA subreddit on the other hand, has a mean of 2.42 while its median is also 1 post per author, with a more skewed distribution. For instance the top poster in Machine learning has 7 out of the total 100 posts whereas the top poster (wewewawa) in TSLA has 19 out of the 46. We should worry about the TSLA subreddit data being biased by this one author who seems to be contributing a major portion of the posts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "24cee524a859595ee85b0018f041bafd5553017bdcc31ab69cf2b1049c4d1d6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
