{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ff2061",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cd28c",
   "metadata": {
    "papermill": {
     "duration": 0.15014,
     "end_time": "2022-05-14T21:23:05.759231",
     "exception": false,
     "start_time": "2022-05-14T21:23:05.609091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDA with Walmart Sales Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e041d2",
   "metadata": {
    "papermill": {
     "duration": 0.127394,
     "end_time": "2022-05-14T21:23:06.801644",
     "exception": false,
     "start_time": "2022-05-14T21:23:06.674250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Business Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80ebbe",
   "metadata": {
    "papermill": {
     "duration": 0.129918,
     "end_time": "2022-05-14T21:23:07.060222",
     "exception": false,
     "start_time": "2022-05-14T21:23:06.930304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Walmart, the retail giant that operates a chain of hypermarkets, wants to understand their weekly sales data, especially the impact from holidays and or big events on the weekly sales data; specifically, Super Bowl, Labor Day, Thanksgiving, and Christmas. In addition, Walmart wants to consider the effect from different macroeconomic/external factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b3f21",
   "metadata": {
    "papermill": {
     "duration": 0.130444,
     "end_time": "2022-05-14T21:23:06.028016",
     "exception": false,
     "start_time": "2022-05-14T21:23:05.897572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8194f",
   "metadata": {
    "papermill": {
     "duration": 0.130444,
     "end_time": "2022-05-14T21:23:06.028016",
     "exception": false,
     "start_time": "2022-05-14T21:23:05.897572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At the end of this session, you will know how to\n",
    "\n",
    "1. Manipulate data of different types using `pandas`\n",
    "1. Visualize data with `matplotlib` and `seaborn` to extract insights \n",
    "1. Perform feature engineering\n",
    "1. Build a pipeline to preprocess data and fit a simple model using `sklearn`\n",
    "\n",
    "*Note: if you see code that's unfamiliar to you, look up for the documentation, and try to understand what it does.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91acd6e9",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121c77b",
   "metadata": {},
   "source": [
    "- Original sales data were collected from 45 stores across the United States; yet for this session, you will first inspect data from three stores and later focus on just store 1. \n",
    "\n",
    "- Each store is of certain type and size, and there are multiple departments in a store. \n",
    "\n",
    "- The dataset has a temporal component, we ignore this mostly in this session and will discuss time series related techniques later in the cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0826745",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:07.590761Z",
     "iopub.status.busy": "2022-05-14T21:23:07.590043Z",
     "iopub.status.idle": "2022-05-14T21:23:09.233960Z",
     "shell.execute_reply": "2022-05-14T21:23:09.232902Z",
     "shell.execute_reply.started": "2022-05-14T20:31:16.167193Z"
    },
    "papermill": {
     "duration": 1.783243,
     "end_time": "2022-05-14T21:23:09.234180",
     "exception": false,
     "start_time": "2022-05-14T21:23:07.450937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # allow multiple outputs in a cell\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba074fc",
   "metadata": {},
   "source": [
    "## Task I: Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c9537",
   "metadata": {
    "papermill": {
     "duration": 0.131091,
     "end_time": "2022-05-14T21:23:09.496169",
     "exception": false,
     "start_time": "2022-05-14T21:23:09.365078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Built on top of `numpy`, `pandas` is one of the most widely used tools in machine learning. Its rich features are used for exploring, cleaning, visualizing, and transforming data.  We need to import the library to access all of its features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419afd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cc900",
   "metadata": {
    "papermill": {
     "duration": 0.131091,
     "end_time": "2022-05-14T21:23:09.496169",
     "exception": false,
     "start_time": "2022-05-14T21:23:09.365078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Use `pd.read_csv` to read `train_comb.csv` that contains weekly sales, metadata, and macroeconomic features from three stores into a `pd.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ac36cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:09.769778Z",
     "iopub.status.busy": "2022-05-14T21:23:09.769010Z",
     "iopub.status.idle": "2022-05-14T21:23:10.331119Z",
     "shell.execute_reply": "2022-05-14T21:23:10.330499Z",
     "shell.execute_reply.started": "2022-05-14T20:31:17.943779Z"
    },
    "papermill": {
     "duration": 0.705534,
     "end_time": "2022-05-14T21:23:10.331295",
     "exception": false,
     "start_time": "2022-05-14T21:23:09.625761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filepath = '../dat/train_comb.csv'\n",
    "data = pd.read_csv(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6093023",
   "metadata": {},
   "source": [
    "Verify that the data is loaded correctly by running `data.head(3)` to see the first few row ( AVOID printing out the entire DataFrame, i.e., `data` or `print(data)`; it might be trivial for small dataset but it can crash your kernel when the dataset is big and slow down the initial data exploration process )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1febad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday  Temperature  Fuel_Price  \\\n",
       "0      1     1  2010-02-05      24924.50      False        42.31       2.572   \n",
       "1      1     1  2010-02-12      46039.49       True        38.51       2.548   \n",
       "2      1     1  2010-02-19      41595.55      False        39.93       2.514   \n",
       "\n",
       "   MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5         CPI  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN  211.096358   \n",
       "1        NaN        NaN        NaN        NaN        NaN  211.242170   \n",
       "2        NaN        NaN        NaN        NaN        NaN  211.289143   \n",
       "\n",
       "   Unemployment Type    Size  \n",
       "0         8.106    A  151315  \n",
       "1         8.106    A  151315  \n",
       "2         8.106    A  151315  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85651101",
   "metadata": {},
   "source": [
    "❓ Question 1:\n",
    "\n",
    "Look at the output to get an idea of what each column is and then write a few sentences describing what you notice about the data. You can also use `data.sample(3)` to draw random samples from the data (hints: number of rows and columns, any missing values? data types of the elements? date ranges of the data collected? etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7274cb6",
   "metadata": {},
   "source": [
    "    YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84e2dd",
   "metadata": {},
   "source": [
    "*Acceptable responses include the number of rows and columns in the dataset, the data types of the elements, how many NaNs there are (and perhaps which columns and/or rows tend to have them), the range of values in each column or other descriptive statistics, some commentary on what this data represents, any initial concerns about how you think we should model this data, or any other commentary you would like to add.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2630bb1",
   "metadata": {
    "papermill": {
     "duration": 0.128082,
     "end_time": "2022-05-14T21:23:13.899405",
     "exception": false,
     "start_time": "2022-05-14T21:23:13.771323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Use `.shape` to inspect the size of the data: sample size and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e6caf",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "(30990, 16)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56d680",
   "metadata": {},
   "source": [
    "For the following task, we focus on Store `1` only, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7dfd053",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store1 = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2e3fa",
   "metadata": {},
   "source": [
    "Retrieve the data from department 9 ( a random choice ) at store 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d606a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store1_dept9 = data_store1[data_store1.Dept == 9] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933093ab",
   "metadata": {},
   "source": [
    "Verify the result using `.head()`, `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store1_dept9.head()\n",
    "data_store1_dept9.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e79bb4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "(143, 16)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7788c7d",
   "metadata": {},
   "source": [
    "Visualize one full year of sales. The data came with dates sorted, but we can make sure of it and then visualize the first 52 data  points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store1_dept9 = data_store1_dept9.sort_values('Date')\n",
    "data_store1_dept9[['Date', 'Weekly_Sales']].iloc[:52]\\\n",
    "    .set_index('Date').plot(rot=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863382f7",
   "metadata": {},
   "source": [
    "❓ Question 2:\n",
    "\n",
    "Do you have any hypotheses about the holidays' impact on the sales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a1b7d",
   "metadata": {},
   "source": [
    "    YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4efef8",
   "metadata": {},
   "source": [
    "**For the purpose of this notebook, we focus on the sales data from Store 1** in DataFrame `df` and is saved in `train_store1.csv`. Let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f836e24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:17.224625Z",
     "iopub.status.busy": "2022-05-14T21:23:17.223574Z",
     "iopub.status.idle": "2022-05-14T21:23:17.230682Z",
     "shell.execute_reply": "2022-05-14T21:23:17.231246Z",
     "shell.execute_reply.started": "2022-05-14T20:31:21.509256Z"
    },
    "papermill": {
     "duration": 0.144233,
     "end_time": "2022-05-14T21:23:17.231443",
     "exception": false,
     "start_time": "2022-05-14T21:23:17.087210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dat/train-store1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a7d25a",
   "metadata": {},
   "source": [
    "Extract week, month, and year information from the raw `Date` column to better manipulate the weekly data later. Pandas comes with powerful features to make this step easy. Reference: [tutorial\n",
    "](https://pandas.pydata.org/docs/getting_started/intro_tutorials/09_timeseries.html). \n",
    "\n",
    "First, use `.dtypes` to check the datatype of the `Date` column. What's the difference between `df[['Date']]` and `df['Date']`?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3324b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7fc60",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "Date    object\n",
    "dtype: object\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d9ad3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:18.082947Z",
     "iopub.status.busy": "2022-05-14T21:23:18.082240Z",
     "iopub.status.idle": "2022-05-14T21:23:18.382486Z",
     "shell.execute_reply": "2022-05-14T21:23:18.381861Z",
     "shell.execute_reply.started": "2022-05-14T20:31:21.534163Z"
    },
    "papermill": {
     "duration": 0.464264,
     "end_time": "2022-05-14T21:23:18.382641",
     "exception": false,
     "start_time": "2022-05-14T21:23:17.918377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.Date=pd.to_datetime(df.Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84ece0",
   "metadata": {},
   "source": [
    "Verify that the `Date` column's datatype has changed as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "786ee438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date    datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Date']].dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20119870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:18.082947Z",
     "iopub.status.busy": "2022-05-14T21:23:18.082240Z",
     "iopub.status.idle": "2022-05-14T21:23:18.382486Z",
     "shell.execute_reply": "2022-05-14T21:23:18.381861Z",
     "shell.execute_reply.started": "2022-05-14T20:31:21.534163Z"
    },
    "papermill": {
     "duration": 0.464264,
     "end_time": "2022-05-14T21:23:18.382641",
     "exception": false,
     "start_time": "2022-05-14T21:23:17.918377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['week'] = df.Date.dt.week\n",
    "df['month'] = df.Date.dt.month \n",
    "df['year'] = df.Date.dt.year "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796da36",
   "metadata": {},
   "source": [
    "Verify that now there are 19 columns in `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6bc64",
   "metadata": {
    "papermill": {
     "duration": 0.133103,
     "end_time": "2022-05-14T21:23:18.998289",
     "exception": false,
     "start_time": "2022-05-14T21:23:18.865186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "❓ Question 3:\n",
    "\n",
    "Last step before we look deeper into the features is to split the data set into training and testing datasets. Discuss: why do we want to perform EDA only on the training data, not the entire dataset? Shouldn't it be the more the better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb470d",
   "metadata": {},
   "source": [
    "    YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ab5c3",
   "metadata": {},
   "source": [
    "*The answer should mention data leakage, and / or overfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382aab82",
   "metadata": {
    "papermill": {
     "duration": 0.133103,
     "end_time": "2022-05-14T21:23:18.998289",
     "exception": false,
     "start_time": "2022-05-14T21:23:18.865186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Split the data into training (80%) and test dataset (20%). Use function `train_test_split` from `scikit-learn` ( a popular library for machine learning in Python ),  and set `random_state` to be 42 for reproducibility ( this is not the best way to do train-test-split due to the temporal nature of the data, however, we will ignore it for now )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e253973d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:19.272142Z",
     "iopub.status.busy": "2022-05-14T21:23:19.271435Z",
     "iopub.status.idle": "2022-05-14T21:23:19.394337Z",
     "shell.execute_reply": "2022-05-14T21:23:19.393604Z",
     "shell.execute_reply.started": "2022-05-14T20:31:21.942121Z"
    },
    "papermill": {
     "duration": 0.26299,
     "end_time": "2022-05-14T21:23:19.394491",
     "exception": false,
     "start_time": "2022-05-14T21:23:19.131501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test =  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78f1046e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T21:23:19.272142Z",
     "iopub.status.busy": "2022-05-14T21:23:19.271435Z",
     "iopub.status.idle": "2022-05-14T21:23:19.394337Z",
     "shell.execute_reply": "2022-05-14T21:23:19.393604Z",
     "shell.execute_reply.started": "2022-05-14T20:31:21.942121Z"
    },
    "papermill": {
     "duration": 0.26299,
     "end_time": "2022-05-14T21:23:19.394491",
     "exception": false,
     "start_time": "2022-05-14T21:23:19.131501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original set  --->  (10244, 19) \n",
      "Training set  --->  (8195, 19) \n",
      "Testing set   --->  (2049, 19)\n"
     ]
    }
   ],
   "source": [
    "print('Original set  ---> ',df.shape,\n",
    "      '\\nTraining set  ---> ',df_train.shape,\n",
    "      '\\nTesting set   ---> ', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74698a5f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "    ```\n",
    "    Original set  --->  (10244, 19) \n",
    "    Training set  --->  (8195, 19) \n",
    "    Testing set   --->  (2049, 19)\n",
    "    ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f539f",
   "metadata": {
    "papermill": {
     "duration": 0.133874,
     "end_time": "2022-05-14T21:23:19.663355",
     "exception": false,
     "start_time": "2022-05-14T21:23:19.529481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Task II: Target, Features, and Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc1d40",
   "metadata": {},
   "source": [
    "We inspect the datatype of column `Date`; now find datatypes for all columns in `df_train` using `.dtypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fcd41a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94a5ba",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Store                    int64\n",
    "Dept                     int64\n",
    "Date            datetime64[ns]\n",
    "Weekly_Sales           float64\n",
    "IsHoliday                 bool\n",
    "Temperature            float64\n",
    "Fuel_Price             float64\n",
    "MarkDown1              float64\n",
    "MarkDown2              float64\n",
    "MarkDown3              float64\n",
    "MarkDown4              float64\n",
    "MarkDown5              float64\n",
    "CPI                    float64\n",
    "Unemployment           float64\n",
    "Type                    object\n",
    "Size                     int64\n",
    "week                     int64\n",
    "month                    int64\n",
    "year                     int64\n",
    "dtype: object\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd8249",
   "metadata": {},
   "source": [
    "Summary statistics provide you with a general understanding of the data. Use method `.describe()`. By default it reports statistics mean, max, min, quantiles for numerical features and counts, unique, mode for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90817aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b4068e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "\tStore\tDept\tWeekly_Sales\tTemperature\tFuel_Price\tMarkDown1\tMarkDown2\tMarkDown3\tMarkDown4\tMarkDown5\tCPI\tUnemployment\tSize\tweek\tmonth\tyear\n",
    "count\t8,195.00\t8,195.00\t8,195.00\t8,195.00\t8,195.00\t2,931.00\t2,424.00\t2,878.00\t2,931.00\t2,931.00\t8,195.00\t8,195.00\t8,195.00\t8,195.00\t8,195.00\t8,195.00\n",
    "mean\t1.00\t44.65\t21,865.28\t68.19\t3.22\t8,045.43\t2,961.55\t1,236.83\t3,683.59\t5,023.69\t216.00\t7.61\t151,315.00\t25.89\t6.47\t2,010.96\n",
    "std\t0.00\t29.95\t27,970.00\t14.16\t0.43\t6,484.49\t8,032.30\t7,830.99\t5,849.69\t3,303.07\t4.33\t0.38\t0.00\t14.19\t3.25\t0.80\n",
    "min\t1.00\t1.00\t-863.00\t35.40\t2.51\t410.31\t0.50\t0.25\t8.00\t554.92\t210.34\t6.57\t151,315.00\t1.00\t1.00\t2,010.00\n",
    "25%\t1.00\t20.00\t3,502.09\t57.79\t2.76\t4,039.39\t40.48\t6.00\t577.14\t3,127.88\t211.57\t7.35\t151,315.00\t14.00\t4.00\t2,010.00\n",
    "50%\t1.00\t38.00\t10,357.32\t69.64\t3.29\t6,154.14\t137.86\t30.23\t1,822.55\t4,325.19\t215.46\t7.79\t151,315.00\t26.00\t6.00\t2,011.00\n",
    "75%\t1.00\t72.00\t31,647.36\t80.48\t3.59\t10,121.97\t1,569.00\t101.64\t3,639.42\t6,222.25\t220.64\t7.84\t151,315.00\t38.00\t9.00\t2,012.00\n",
    "max\t1.00\t99.00\t203,670.47\t91.65\t3.91\t34,577.06\t46,011.38\t55,805.51\t32,403.87\t20,475.32\t223.44\t8.11\t151,315.00\t52.00\t12.00\t2,012.0\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9561ad",
   "metadata": {},
   "source": [
    "❓ Question 4:\n",
    "\n",
    "Inspect the output, what are some of your observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b7e01",
   "metadata": {},
   "source": [
    "    YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e70f45",
   "metadata": {},
   "source": [
    "Are there any missing values? Use `.isna()` and `.sum()` to show the number of missing values from each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89849ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5806930",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "Store              0\n",
    "Dept               0\n",
    "Date               0\n",
    "Weekly_Sales       0\n",
    "IsHoliday          0\n",
    "Temperature        0\n",
    "Fuel_Price         0\n",
    "MarkDown1       5264\n",
    "MarkDown2       5771\n",
    "MarkDown3       5317\n",
    "MarkDown4       5264\n",
    "MarkDown5       5264\n",
    "CPI                0\n",
    "Unemployment       0\n",
    "Type               0\n",
    "Size               0\n",
    "week               0\n",
    "month              0\n",
    "year               0\n",
    "dtype: int64\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d2c82",
   "metadata": {},
   "source": [
    "What do you think the target variable is in this problem? Assign the column name to `target` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "632db34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c325e9",
   "metadata": {},
   "source": [
    "Visualize the distribution of target variable using `distplot()` from library `seaborn` ( Why seaborn? Check out a comparison between Matplotlib and Seaborn [here](https://analyticsindiamag.com/comparing-python-data-visualization-tools-matplotlib-vs-seaborn/) ). Anything here you observe but the output from `.describe` does not make obvious? Does it follow a normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(df_train[target],bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bb611",
   "metadata": {},
   "source": [
    "Notice that there exists nonpositive weekly sales. How many of rows are there that the weekly sales are negative or 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ff614",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train[target] <= 0).sum() # Expected Output: 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5cb2f",
   "metadata": {},
   "source": [
    "What percentage is the negative and zero sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ca3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0f0ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "`0.0015863331299572911` or `0.16%`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f87f2",
   "metadata": {},
   "source": [
    "After communicating your findings, the stakeholders confirm that you can remove these data entries for now and they are launching an investigation by analysts and data engineers. \n",
    "\n",
    "Now remove them from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_train[target] > 0\n",
    "df_train = # YOUR CODE HERE\n",
    "df_train.shape # Expected Output: (8182, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2db90",
   "metadata": {},
   "source": [
    "Let's move on to features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d222",
   "metadata": {},
   "source": [
    "Though almost all the come through as numerical, should they all be treated as numerical features? Let's inspect the number of unique values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(col, df[col].nunique())for col in df_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf52e83",
   "metadata": {},
   "source": [
    "`Temperature`, `CPI`, `Unemployment`, `Fuel_Price` are continuous. Those tie to the second business objective. Let us put these four into a list and store it in `external_factors`. From earlier, we noticed that `MarkDownx` columns contain some missing values, we will treat them in a later task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66c9ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_factors = ['Temperature','CPI','Unemployment', 'Fuel_Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5737d6",
   "metadata": {},
   "source": [
    "Visualize Temperature in a box plot, what do you think is the advantage of a box plot over a histogram? You can use `pd.DataFrame.boxplot()`, set the figure size as (6, 4), and turn off the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e747c",
   "metadata": {},
   "source": [
    "❓ Question 5:\n",
    "\n",
    "Visualize Temperature in a box plot, what do you think the advantage of a box plot over histogram? \n",
    "\n",
    "HINT: You can use `pd.DataFrame.boxplot()`, set the figure size as (6, 4), and turn off the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e2c38",
   "metadata": {},
   "source": [
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98178024",
   "metadata": {},
   "source": [
    "Let's visualize all four numerical features in both density plot and box plot. Note any observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print('\\033[1mNumeric Features Distribution'.center(100))\n",
    "\n",
    "figsize = (12, 4)\n",
    "\n",
    "n=len(external_factors)\n",
    "colors = ['g', 'b', 'r', 'y', 'k']\n",
    "\n",
    "# histogram\n",
    "plt.figure(figsize=figsize)\n",
    "for i in range(len(external_factors)):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    sns.distplot(df_train[external_factors[i]],\n",
    "                 bins=10, \n",
    "                 color = colors[i])\n",
    "plt.tight_layout();\n",
    "\n",
    "# boxplot\n",
    "plt.figure(figsize=figsize)\n",
    "for i in range(len(external_factors)):\n",
    "    plt.subplot(1,n,i+1)\n",
    "    df_train.boxplot(external_factors[i], grid=False)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c8bc4",
   "metadata": {},
   "source": [
    "We will investigate the impacts of the external factors later. Now let's scan through the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b165a4",
   "metadata": {},
   "source": [
    "`Store`, `Type`, and `Size` each has only one unique value, offering no information, we can safely ignore them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8f257",
   "metadata": {},
   "source": [
    "We extracted `year`, `month`, and `week` from  `Date`, thus `Date` is redundant; but it is easy to find the date range in the training dataset using `Date`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Date'].min(), df_train['Date'].max() # Expected Output: (Timestamp('2010-02-05 00:00:00'), Timestamp('2012-10-26 00:00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86745e55",
   "metadata": {},
   "source": [
    "Our training data ranges from 5th of February 2010 to 26th of October 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93438e",
   "metadata": {},
   "source": [
    "It makes more sense to treat `year`, `month`, `week` as categorical, more accurately ordinal; and the boolean feature `IsHoliday` can be considered as categorical, so can `Dept`. Let's put these column names into a list `categoricalFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "668b91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalFeatures = ['year','month','week','IsHoliday', 'Dept']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c1cdd",
   "metadata": {},
   "source": [
    "For the categorical features, we are more interested in the frequency of each value, use `pd.Series.value_counts` to see how many rows where `IsHoliday` is true and false respectively ( Data imbalance is the norm )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9fc85",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "False    7586\n",
    "True      596\n",
    "Name: IsHoliday, dtype: int64\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c091c4",
   "metadata": {},
   "source": [
    "Visualize the distribution of `month`; use `sns.countplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2077ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the categorical features \n",
    "\n",
    "print('\\033[1mVisualising Categorical Features:'.center(100))\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "for i in range(len(categoricalFeatures)):\n",
    "    plt.subplot(6,1,i+1)\n",
    "    sns.countplot(df_train[categoricalFeatures[i]])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b243b",
   "metadata": {},
   "source": [
    "❓ Question 6: \n",
    "\n",
    "Discuss with your pair programming partner: There is less data in 2012 than the previous two years, did the sale drop from previous years? Does it affect what we see in the plots for month and week? Does the plot below clarify it to some degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fe3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_train, x=\"week\", y=\"Weekly_Sales\",  style='year');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9e53a",
   "metadata": {},
   "source": [
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9df48",
   "metadata": {},
   "source": [
    "## Task III: Impact from Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d13d4",
   "metadata": {},
   "source": [
    "The first business objective is to understand the impact of holidays on weekly sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f937f9f",
   "metadata": {},
   "source": [
    "There is a flag provided for us: `IsHoliday`, let's calculate the average weekly sales for holiday weeks and non-holiday weeks, respectively. For this, we will use `.groupBy` and `.mean()`. Are holiday sales higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0cde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06d48a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "IsHoliday\n",
    "False   21,756.05\n",
    "True    23,737.05\n",
    "Name: Weekly_Sales, dtype: float64\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e1c70",
   "metadata": {},
   "source": [
    "But we would like to understand it at more granular level, remember [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson's_paradox)? To save some time,  date mapping are identified for the training data\n",
    "\n",
    "- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12\n",
    "- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12\n",
    "- Thanksgiving: 26-Nov-10, 25-Nov-11\n",
    "- Christmas: 31-Dec-10, 30-Dec-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfee7f9",
   "metadata": {},
   "source": [
    "We create a flag for each holiday to help you analyze weekly sale by each holiday type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6193c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "superbowl_mask = df_train['Date'].isin(['2010-02-12', '2011-02-11', '2012-02-10'])\n",
    "laborday_mask = df_train['Date'].isin(['2010-09-10', '2011-09-09','2012-09-07'])\n",
    "thanksgiving_mask = df_train['Date'].isin(['2010-11-26', '2011-11-25'])\n",
    "christmas_mask = df_train['Date'].isin(['2010-12-31', '2011-12-30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "125ace7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['superbowl'] = superbowl_mask\n",
    "df_train['laborday'] = laborday_mask\n",
    "df_train['thanksgiving'] =thanksgiving_mask\n",
    "df_train['christmas'] = christmas_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936a755",
   "metadata": {},
   "source": [
    "Run the next cell to see 1) how many weekly sales fell on Christmas (does it make sense? what did we not account for?) 2) what is the average weekly sales stratified by whether it is Christmas week or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(['christmas'])\\\n",
    "        .agg(count = ('christmas', 'size'), \n",
    "             avg_weekly_sales= ('Weekly_Sales','mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af8383",
   "metadata": {},
   "source": [
    "Perform the same for the other three holidays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = ['superbowl', 'laborday', 'thanksgiving', 'christmas']\n",
    "for holiday in holidays:\n",
    "    summary_stats = df_train.groupby([holiday])\\\n",
    "        # YOUR CODE HERE\n",
    "        # YOUR CODE HERE\n",
    "    print(summary_stats)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869aaae",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "           count  avg_weekly_sales\n",
    "superbowl                         \n",
    "False       8001         21,845.80\n",
    "True         181         24,311.98\n",
    "\n",
    "          count  avg_weekly_sales\n",
    "laborday                         \n",
    "False      8007         21,884.35\n",
    "True        175         22,632.78\n",
    "\n",
    "              count  avg_weekly_sales\n",
    "thanksgiving                         \n",
    "False          8067         21,813.97\n",
    "True            115         27,959.84\n",
    "\n",
    "           count  avg_weekly_sales\n",
    "christmas                         \n",
    "False       8057         21,921.06\n",
    "True         125         20,565.56\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6455d9",
   "metadata": {},
   "source": [
    "Without hypothesis testing and by only eyeballing, it seems like Super Bowl and Thanksgiving has a positive impact on the weekly sales for Store 1 in this training dataset. \n",
    "Discuss with your teammate, are you surprised that during Christmas, sales at Walmart do not go up? Holiday effect, if causal, happened most during Thanksgiving weeks, is this something you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482d2d4",
   "metadata": {},
   "source": [
    "We have been ignoring `Dept`, let's take a look at the plot below showing the weekly sales by department in 2011. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.scatterplot(data=df_train[df_train.year==2011], x = 'Dept', y= target, hue='IsHoliday');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23615096",
   "metadata": {},
   "source": [
    "Dept 72 has a very unusual high weekly sales during the holiday week, but we will need more data to understand if this is data issue, outlier, or special event. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16a23e",
   "metadata": {},
   "source": [
    "## Task IV: Visualize Relationship between Macroeconomic & External Factors and Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_train, x=\"Fuel_Price\", y=\"Weekly_Sales\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_train, x=\"Temperature\", y=\"Weekly_Sales\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_train, x=\"CPI\", y=\"Weekly_Sales\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_train, x=\"Unemployment\", y=\"Weekly_Sales\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a2544",
   "metadata": {},
   "source": [
    "By eyeballing, do you find strong evidence that those are correlated with Walmart's weekly sales? Do you think `lineplot` is an appropriate plot for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185fcd1d",
   "metadata": {},
   "source": [
    "Lastly, we calculate the spearman correlations among target and external factors and verify that there is no strong linear correlation between the target variable and these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "df_train_reduced = df_train[[target] + external_factors]\n",
    "corr = df_train_reduced.corr(method='spearman')\n",
    "heatmap = sns.heatmap(corr.sort_values(by=target, ascending=False),\n",
    "                      vmin=-1, vmax=1, annot=True, fmt='.1g', cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with Sales Price', fontdict={'fontsize':12}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b4c79",
   "metadata": {},
   "source": [
    "## Task V: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ecd53",
   "metadata": {},
   "source": [
    "\"*Feature Engineering encapsulates various data engineering techniques such as selecting relevant features, handling missing data, encoding the data, and normalizing it. It is one of the most crucial tasks and plays a major role in determining the outcome of a model.*\" [Ref](https://www.analyticsvidhya.com/blog/2021/10/a-beginners-guide-to-feature-engineering-everything-you-need-to-know/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef295e7",
   "metadata": {},
   "source": [
    "One part of feature engineering is to create new features from given data, like `thanksgiving` column earlier was derived from `Date`. Common techniques for tabular data include to add summary statistics of the numerical features such as mean and standard deviation, to create new features from the interaction of multiple features, etc. In this task, however, we will work on handling missing data, normalizing numerical features, and encoding categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112542e",
   "metadata": {},
   "source": [
    "First, missing data. Missing value treatment is crucial, yet not trivial. Take a read on [Tackling Missing Value in Dataset](https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/) for detailed explanation. Features with nulls or wrong values (e.g., negative fuel price) needs to be imputed or removed. \n",
    "\n",
    "- Do you want to keep the features with missing value? Discuss the trade offs\n",
    "- If answer to the first question is yes, then how do you want to impute them? Discuss the trade offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42d205",
   "metadata": {},
   "source": [
    "From ealier steps, we observed that only the markdown columns contain missing values, yet we do not have more information on what those values are for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57607650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns[df_train.isna().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff8e3a",
   "metadata": {},
   "source": [
    "For each column, find out the percentage of the data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc0652",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "md_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "for col in ['MarkDown'+str(i) for i in range(1,6)]:\n",
    "    perc_missing =  # YOUR CODE HERE; perc_missing:float\n",
    "    print (f'{col}: {perc_missing:.0%} is missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211b2c4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected Output</summary>\n",
    "\n",
    "```\n",
    "MarkDown1: 64% is missing\n",
    "MarkDown2: 70% is missing\n",
    "MarkDown3: 65% is missing\n",
    "MarkDown4: 64% is missing\n",
    "MarkDown5: 64% is missing\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f8286",
   "metadata": {},
   "source": [
    "The majority of the markdown fields are missing. This is where, again, we need to communicate with the stakeholders to understand what the data measure, how the data was collected and then determine our strategy from there. Since we want to understand the impacts of `MarkDownx` on weekly sales, we will keep the features and impute the missing values.  We have learned that there are tradeoffs with how we treat missing values and that our choice of imputation can be significantly impacted by extreme values and the amount of the missing data.  We choose to impute with the median here to mitigate these negative impacts. Use `.fillna()` to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d7f45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE # this works for smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train.isna().sum() != 0).sum() # sanity check: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5cb0c1",
   "metadata": {},
   "source": [
    "Visualize the distributions for those markdown fields after imputations, are they normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "for i in range(len(md_cols)):\n",
    "    plt.subplot(1,len(md_cols),i+1)\n",
    "    sns.distplot(df_train[md_cols[i]],\n",
    "                 hist_kws=dict(linewidth=2),\n",
    "                 bins=10, \n",
    "                 color = colors[i])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388fccb",
   "metadata": {},
   "source": [
    "Note that missing values are different from outliers. Outliers, on the other hand, are feature values that are rare in nature. They can unncessarily skew the data and causes problem for modeling. Outlier treatment involves removing or imputing such values. One popular approach to identify outliers is IQR; that is, data points that lie 1.5 times of IQR above Q3 (third quartile) and below Q1 (first quartile) are outliers. Take a read on [Detecting and Treating Outliers](https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/). We will leave it as an optional exercise for you to identify outliers using IQR, and replace the outliers with the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303270c5",
   "metadata": {},
   "source": [
    "Now let's see how we normalize the data. For numerical features it means scaling the features to be of similar range. This step is crucial for machine learning algorithms that calculate distances between data (e.g., read [The Importance of Feature Scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df6fd06",
   "metadata": {},
   "source": [
    "For this task, of the external features, let's keep Temperature since it is the most linearly correlated with the target variable, though very weak and negative ( feature selection ). In addition, we include one markdown field. Since neither seems to follow normal distributions, it is safer to use `MinMaxScaler` from `sklearn.preprocessing` to transform features by scaling each feature to a given range (See discussion on [Normalization vs Standardization](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0deef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numericalFeatures = ['Temperature', 'MarkDown1']\n",
    "df_train_num = df_train[numericalFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69880265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_num.describe() # Check the summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb15069",
   "metadata": {},
   "source": [
    "Instantiate a MinMaxScaler and fit using `df_train_num`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "621dc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912f6ee",
   "metadata": {},
   "source": [
    "Now transform training data `df_train_num` and store the resulting nparray in `train_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d2f98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm = scaler.transform(df_train_num) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cf97e",
   "metadata": {},
   "source": [
    "Verify that both columns now have minimum 0 and maximum 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16417a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_norm, columns=df_train_num.columns).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17401c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>MarkDown1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8,182.00</td>\n",
       "      <td>8,182.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temperature  MarkDown1\n",
       "count     8,182.00   8,182.00\n",
       "mean          0.58       0.19\n",
       "std           0.25       0.12\n",
       "min           0.00       0.00\n",
       "25%           0.40       0.17\n",
       "50%           0.61       0.17\n",
       "75%           0.80       0.17\n",
       "max           1.00       1.00"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48b3d6",
   "metadata": {},
   "source": [
    "Let's turn to categorical fatures. So far most, if not all Python packages for modeling do not accept strings as input; thus encoding the categorical value to numerical value is a necessary step. Here, let's apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) on `Dept` and `IsHoliday`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32890ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categoricalFeatures = ['Dept', 'IsHoliday']\n",
    "df_train_cat = df_train[categoricalFeatures]\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',sparse = False).fit(df_train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af8c7e",
   "metadata": {},
   "source": [
    "Transform the categorical features using one hote encoding `ohe`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd52f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohe.shape, df_train_cat.shape # Expected Output: ((8182, 79), (8182, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bc5a4",
   "metadata": {},
   "source": [
    "The number of columns explodes from 2 to 79. \n",
    "\n",
    "Lastly we merge the processed numerical features with the processed categorical features using `hstack` in `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d7b615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.hstack([train_norm, train_ohe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a93c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape # sanity check: (8182, 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de66122",
   "metadata": {},
   "source": [
    "What about the test data? Yes you need to apply the same treatments. We spare some copy + paste + edit and see how this can be done when we introduce `pipeline` next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32bead",
   "metadata": {},
   "source": [
    "## Task VI: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9d049",
   "metadata": {},
   "source": [
    "Even with less than 20 features in our dataset, there are many many possibilities that you can preprocessing the data. There is no one-fits-all approach; often you will find yourself experimenting with many combinations to achieve better modelling performance: Should I apply normalization or standardization? Do I remove the outliers or should I impute them? Do I impute the missing values with median or mean or 0? Answers to many of these questions is \"It depends.\" (Have you heard [Graduate Student Descent](https://sciencedryad.wordpress.com/2014/01/25/grad-student-descent/)?) That means trial-and-error and it is not efficient to produce a notebook each time when you need to try something slightly different. You will get lost quickly. Pipeline is one useful tool. \n",
    "\n",
    "Not only does Pipeline help streamline the process, keep the code modular, but also reduces the possibility of introducing errors/bugs. In this task, we build the pipeline following the strategies used in the last task, run a simple linear regression model, and print out the model's performance. Note there is minimal code required for you to implement, the key is to understand each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcc7db",
   "metadata": {},
   "source": [
    "To avoid confusion, let's read the data again directly from `train-store1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4da8cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dat/train-store1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c6682",
   "metadata": {},
   "source": [
    "Separating the target `y` from the features `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90b50b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=target), df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519cffb2",
   "metadata": {},
   "source": [
    "Import `Pipeline` from submodule `sklearn.pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88317c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ee955",
   "metadata": {},
   "source": [
    "Now we build a transformer for numerical features following two steps: impute the missing values with the feature median (use `SimpleImputer`), followed by normalization (use `MinMaxScaler`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0080726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "numeric_features = ['CPI', 'MarkDown1']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "    # YOUR CODE HERE\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd31c6",
   "metadata": {},
   "source": [
    "For categorical features, we apply one hot encoding `OneHotEncoder` ( there are many other options; see [Scikit-learn documentation](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1307a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Dept', 'IsHoliday']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed811b",
   "metadata": {},
   "source": [
    "Piece the `numeric_transformer` and `categorical_transformer` using `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9162e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a2dbc",
   "metadata": {},
   "source": [
    "Lastly, let's append the regression model to preprocessing pipeline to complete a full prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69eb8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", LinearRegression())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226da05",
   "metadata": {},
   "source": [
    "The pipepline has been built! The rest is to \n",
    "- split the data into training and testing sets\n",
    "- apply the pipeline to the training data\n",
    "- obtain the prediction performance on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08ba7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64369e9f",
   "metadata": {},
   "source": [
    "Let's run the prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb720cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63956599",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"model score: %.3f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254275d1",
   "metadata": {},
   "source": [
    "Optional: Discuss what type of [Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection) strategy you would use to select the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b9f63",
   "metadata": {},
   "source": [
    "## Automating EDA\n",
    "\n",
    "In this exercise, you have learned the manual way to perform EDA.  Doing EDA manually has the benefits of customization, but is also highly repetitive.  For this reason, a lot of EDA can easily be automated!  In automating our EDA, we can get to know our data more quickly and spend more time on feature engineering and modeling.  Let's check out a library called [SweetViz](https://github.com/fbdesignpro/sweetviz) to see how we can automate EDA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ba419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "orig_data_report = sv.analyze(df)\n",
    "orig_data_report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e143f",
   "metadata": {},
   "source": [
    "1. Click on a feature to tab to explore the feature in more detail.\n",
    "1. Notice that `SweetViz` calculates the descriptive stats for each feature, along with its missing and duplicate value stats.\n",
    "1. Notice that `SweetViz` helps to detect numerical vs categorical datatypes.\n",
    "1. Click on the `ASSOCIATIONS` tab to explore associations/correlations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d63c8f",
   "metadata": {},
   "source": [
    "### Prefer a browswer experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab635869",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_report.show_html('orig_data_report.html', open_browser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469ed93",
   "metadata": {},
   "source": [
    "### Now let's have a look at a comparison report of our train and test datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_report = sv.compare([X_train, 'Train'], [X_test, 'Test'])\n",
    "compare_report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d54a44",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba537f84",
   "metadata": {},
   "source": [
    "- EDA, like other parts of machine learning, is an iterative process, NOT linear.\n",
    "- This analysis is far from being comprehensive; rather it is a starting point. \n",
    "- There does not exist one \"standard\" way to perform EDA. You should always keep business objectives in mind and perform analysis as seen fit. It is one of those skills that grows with lots of practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37a7e2",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deab61a",
   "metadata": {},
   "source": [
    "1. Original dataset is from [kaggle: wallmart sales forecast datasets](https://www.kaggle.com/datasets/iamprateek/wallmart-sales-forecast-datasets)\n",
    "2. Notebook: [craking the walmart sales forecasting challenge](https://www.kaggle.com/code/fernandol/cracking-the-walmart-sales-forecasting-challenge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 659.006746,
   "end_time": "2022-05-14T21:33:54.545503",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-14T21:22:55.538757",
   "version": "2.3.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "620px",
    "left": "56px",
    "top": "110px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "c57794392b841cffd8686d5c4548e4e2ec78521f49300d60954d1380f1b4bd1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
